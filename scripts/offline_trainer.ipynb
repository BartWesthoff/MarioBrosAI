{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2d06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from model import BTRNetwork\n",
    "\n",
    "\n",
    "\n",
    "def agent_action(filtered_keys):\n",
    "    sprint = filtered_keys.get(\"B\", False)\n",
    "    move_right = filtered_keys.get(\"Right\", False)\n",
    "    move_left = filtered_keys.get(\"Left\", False)\n",
    "\n",
    "    jump = filtered_keys.get(\"A\", False)\n",
    "    crouch = filtered_keys.get(\"Down\", False)\n",
    "    airborne = filtered_keys.get(\"One\", False)\n",
    "    sprint_left = move_left and sprint\n",
    "    sprint_right = move_right and sprint\n",
    "    jump_left = (move_left or sprint_left) and jump\n",
    "    jump_right = (move_right or sprint_right) and jump\n",
    "    stand_still = not any([move_right, move_left, jump, crouch, airborne, sprint_left, sprint_right, jump_left, jump_right])\n",
    "    return {\n",
    "\n",
    "        \"jump\": jump,\n",
    "        \"crouch\": crouch,\n",
    "        \"airborne\": airborne,\n",
    "        \"sprint_left\": sprint_left,\n",
    "        \"sprint_right\": sprint_right,\n",
    "        \"jump_left\": jump_left,\n",
    "        \"jump_right\": jump_right,\n",
    "        \"move_right\": move_right,\n",
    "        \"move_left\": move_left,\n",
    "        \"none\": stand_still,\n",
    "    }\n",
    "\n",
    "# --- Constants ---\n",
    "ACTION_KEYS = [\"jump\", \"crouch\", \"airborne\", \"sprint_left\", \"sprint_right\",\n",
    "               \"jump_left\", \"jump_right\", \"move_right\", \"move_left\", \"none\"]\n",
    "\n",
    "\n",
    "\n",
    "ACTION_TO_INDEX = {action: idx for idx, action in enumerate(ACTION_KEYS)}\n",
    "NUM_ACTIONS = len(ACTION_KEYS)\n",
    "\n",
    "# --- Dataset ---\n",
    "class MarioDataset(Dataset):\n",
    "    def __init__(self, image_folder, movements, frame_window=4):\n",
    "        self.image_folder = image_folder\n",
    "        self.movements = movements\n",
    "        self.frame_window = frame_window\n",
    "        self.frames = sorted(movements.keys())\n",
    "        self.valid_indices = list(range(frame_window, len(self.frames)))\n",
    "\n",
    "\n",
    "        # self.transform = transforms.Compose([\n",
    "        #     transforms.Resize((84, 84)),\n",
    "        #     transforms.ToTensor(),\n",
    "        # ])\n",
    "\n",
    "    # use custom process instead of torchvision transforms, this way we dont need to install torchvision\n",
    "    # for only its transforms. They should effectively do the same thing\n",
    "    def process_image(self, img, resize_to=(140, 114)):\n",
    "        img = img.resize(resize_to, Image.Resampling.LANCZOS)\n",
    "        img_array = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        img_tensor = torch.tensor(img_array).unsqueeze(0)  # Add channel dimension (1, H, W)\n",
    "\n",
    "        return img_tensor # no division (/255) needed because model does it, TODO: maybe move it to procesing instead of model\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.valid_indices[idx]\n",
    "        imgs = []\n",
    "        for i in range(idx - self.frame_window, idx):\n",
    "            frame_key = self.frames[i]\n",
    "            img_path = os.path.join(self.image_folder, f\"d_{frame_key}.png\")\n",
    "            img = Image.open(img_path).convert(\"L\")\n",
    "            imgs.append(self.process_image(img))\n",
    "\n",
    "\n",
    "\n",
    "        state = torch.cat(imgs, dim=0)  # [3*4, 84, 84]\n",
    "        #print(\"state shape\", state.shape)\n",
    "\n",
    "        frame_key = self.frames[idx]\n",
    "        keys = {k: v for k, v in self.movements[frame_key].items() if k != \"state\"}\n",
    "        actions_dict = agent_action(keys)\n",
    "        #print(\"action-dict\", actions_dict)\n",
    "\n",
    "        # TODO check if this is always correct, I think so\n",
    "        # idea here to get the valid action, we take the last if there are more\n",
    "        # e.g. jump_right is after jump and move/sprint right\n",
    "        # latest action counts\n",
    "        for a, value in reversed(actions_dict.items()):\n",
    "            if value:\n",
    "                action = a\n",
    "\n",
    "\n",
    "        reward = self.movements[frame_key][\"state\"].get(\"reward\", 0.0)\n",
    "\n",
    "        # Next frame\n",
    "        if idx + 1 < len(self.frames):\n",
    "            next_frame_key = self.frames[idx + 1]\n",
    "            next_img_path = os.path.join(self.image_folder, f\"d_{next_frame_key}.png\")\n",
    "            next_img = Image.open(next_img_path).convert(\"L\")\n",
    "            next_state = torch.cat([\n",
    "                *imgs[1:],\n",
    "                self.process_image(next_img)\n",
    "            ], dim=0)\n",
    "        else:\n",
    "            next_state = state\n",
    "\n",
    "        return state, action, reward, next_state\n",
    "    \n",
    "\n",
    "# -- testing dataset functionality --\n",
    "\n",
    "\n",
    "# TODO: change the paths here to the ones you have\n",
    "dataset = MarioDataset(\n",
    "    image_folder=\"../experiment/data/screenshots\",\n",
    "    movements=json.load(open(\"../experiment/data/movements.json\")),\n",
    "    frame_window=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f3917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 114, 140])\n"
     ]
    }
   ],
   "source": [
    "data0 = dataset[0]\n",
    "print(data0[0].shape)  # [3*4, 84, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe89187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def train(dataset, epochs=1, batch_size=32, learning_rate=0.0001, gamma=0.99):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = BTRNetwork(num_actions=NUM_ACTIONS).to(device)\n",
    "    model.train()\n",
    "\n",
    "    target_model = BTRNetwork(num_actions=NUM_ACTIONS).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i, (states, actions, rewards, next_states) in enumerate(dataloader):\n",
    "\n",
    "            #print(type(states), type(actions), type(rewards), type(next_states))\n",
    "                \n",
    "    \n",
    "            actions_discrete = [ACTION_TO_INDEX[a] for a in actions]\n",
    "            actions = torch.tensor(actions_discrete, dtype=torch.long).to(device)\n",
    "\n",
    "            states = states.float().to(device)\n",
    "            #print(\"states shape\", states.shape)\n",
    "            rewards = rewards.float().to(device)\n",
    "            next_states = next_states.float().to(device)\n",
    "\n",
    "            # Configs for Munchausen RL\n",
    "            alpha = 0.9  # log-policy scaling\n",
    "            tau = 0.03   # softmax temperature\n",
    "            clip_min = -1.0  # munchausen term clipped minimum\n",
    "\n",
    "            # Get current Q-values\n",
    "            q_values = model(states)\n",
    "            #print(\"q_values shape\", q_values.shape)\n",
    "            q_values_mean = q_values.mean(dim=1)\n",
    "            #print(\"q_values_mean shape\", q_values_mean.shape)\n",
    "            q_taken = q_values_mean.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            #print(\"q_taken shape\", q_taken.shape)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q = target_model(next_states)\n",
    "                #print(\"next_q shape\", next_q.shape)\n",
    "                next_q_mean = next_q.mean(dim=1)\n",
    "                #print(\"next_q_mean shape\", next_q_mean.shape)\n",
    "                next_probs = F.softmax(next_q_mean / tau, dim=1)       # π(a|s')\n",
    "                #print(\"next_probs shape\", next_probs.shape)\n",
    "                next_v = (next_q_mean * next_probs).sum(dim=1)  # Shape: [32]\n",
    "                #print(\"next_v shape\", next_v.shape)\n",
    "\n",
    "                # Softmax over current Q-values (for log π(a|s))\n",
    "                current_probs = F.softmax(q_values_mean / tau, dim=1)\n",
    "                log_policy = torch.log(current_probs + 1e-8)       # [B, A]\n",
    "                log_pi_a = log_policy.gather(1, actions.unsqueeze(1)).squeeze(1)  # log π(a|s)\n",
    "                munchausen_term = alpha * torch.clamp(log_pi_a, min=clip_min)\n",
    "\n",
    "                # Munchausen target\n",
    "                target_q = rewards + munchausen_term + gamma * next_v\n",
    "\n",
    "            loss = loss_fn(q_taken, target_q)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # After each epoch\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11963593",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model, losses) = train(dataset, epochs=5, batch_size=32, learning_rate=0.0004, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49ae5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[808.0996704101562, 122.07223510742188, 132.61505126953125, 88.66462707519531, 1052.9410400390625, 57.21360778808594, 367.99700927734375, 732.253662109375, 111.39972686767578, 231.74403381347656, 90.65411376953125, 80.9020004272461, 484.17669677734375, 1977.562744140625, 61.17024230957031, 296.0118713378906, 330.04791259765625, 225.67422485351562, 61.29290771484375, 348.9710388183594, 20.270885467529297, 56.10218811035156, 55.152156829833984, 66.62965393066406, 321.12969970703125, 700.02734375, 130.2519989013672, 452.03082275390625, 252.7568359375, 58.86053466796875, 708.6806030273438, 121.07814025878906, 28.510343551635742, 557.5345458984375, 403.33447265625, 124.31763458251953, 66.4805679321289, 65.92271423339844, 24.800804138183594, 103.87236022949219, 46.56050109863281, 853.0126953125, 33.31560516357422, 44.231170654296875, 67.92382049560547, 111.99029541015625, 120.53428649902344, 270.84564208984375, 65.20271301269531, 120.75869750976562, 510.658203125, 103.84513092041016, 53.8133430480957, 98.6705551147461, 287.68292236328125, 31.058380126953125, 281.578125, 39.18560791015625, 612.220458984375, 40.229888916015625, 573.4374389648438, 193.02005004882812, 35.274131774902344, 444.283935546875, 24.931987762451172, 142.1520538330078, 227.19749450683594, 76.37242126464844, 503.7659912109375, 27.080812454223633, 43.222965240478516, 49.81461715698242, 153.14096069335938, 23.076536178588867, 423.41363525390625, 27.19536590576172, 236.01380920410156, 378.3586120605469, 150.0376434326172, 107.94368743896484, 30.556625366210938, 51.86249542236328, 143.92042541503906, 46.595123291015625, 130.78329467773438, 145.39398193359375, 11.29925537109375, 401.70599365234375, 23.57565689086914, 57.65404510498047, 539.5931396484375, 47.111995697021484, 473.6903076171875, 51.38450622558594, 155.2643585205078, 93.03846740722656, 16.416933059692383, 74.79208374023438, 59.61961364746094, 133.89581298828125, 355.51641845703125, 74.05426788330078, 116.0169906616211, 45.8213996887207, 324.88775634765625, 121.73455047607422, 148.3759765625, 54.63960266113281, 187.10768127441406, 55.11627960205078, 179.20574951171875, 429.2330627441406, 161.99729919433594, 38.69960021972656, 216.4650421142578, 68.38124084472656, 83.03953552246094, 125.61276245117188, 32.41672134399414, 465.7159423828125, 245.04318237304688, 27.8084659576416, 175.70277404785156, 87.60530853271484, 551.7315673828125, 121.13613891601562, 56.19389343261719, 154.0744171142578, 204.11611938476562, 26.84239387512207]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "#torch.save(model.state_dict(), \"btr_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
