{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2d06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from model import BTRNetwork\n",
    "\n",
    "\n",
    "\n",
    "def agent_action(filtered_keys):\n",
    "    sprint = filtered_keys.get(\"B\", False)\n",
    "    move_right = filtered_keys.get(\"Right\", False)\n",
    "    move_left = filtered_keys.get(\"Left\", False)\n",
    "\n",
    "    jump = filtered_keys.get(\"A\", False)\n",
    "    crouch = filtered_keys.get(\"Down\", False)\n",
    "    airborne = filtered_keys.get(\"One\", False)\n",
    "    sprint_left = move_left and sprint\n",
    "    sprint_right = move_right and sprint\n",
    "    jump_left = (move_left or sprint_left) and jump\n",
    "    jump_right = (move_right or sprint_right) and jump\n",
    "    stand_still = not any([move_right, move_left, jump, crouch, airborne, sprint_left, sprint_right, jump_left, jump_right])\n",
    "    return {\n",
    "\n",
    "        \"jump\": jump,\n",
    "        \"crouch\": crouch,\n",
    "        \"airborne\": airborne,\n",
    "        \"sprint_left\": sprint_left,\n",
    "        \"sprint_right\": sprint_right,\n",
    "        \"jump_left\": jump_left,\n",
    "        \"jump_right\": jump_right,\n",
    "        \"move_right\": move_right,\n",
    "        \"move_left\": move_left,\n",
    "        \"none\": stand_still,\n",
    "    }\n",
    "\n",
    "# --- Constants ---\n",
    "ACTION_KEYS = [\"jump\", \"crouch\", \"airborne\", \"sprint_left\", \"sprint_right\",\n",
    "               \"jump_left\", \"jump_right\", \"move_right\", \"move_left\", \"none\"]\n",
    "\n",
    "\n",
    "\n",
    "ACTION_TO_INDEX = {action: idx for idx, action in enumerate(ACTION_KEYS)}\n",
    "NUM_ACTIONS = len(ACTION_KEYS)\n",
    "\n",
    "# --- Dataset ---\n",
    "class MarioDataset(Dataset):\n",
    "    def __init__(self, image_folder, movements, frame_window=4):\n",
    "        self.image_folder = image_folder\n",
    "        self.movements = movements\n",
    "        self.frame_window = frame_window\n",
    "        self.frames = sorted(movements.keys())\n",
    "        self.valid_indices = list(range(frame_window, len(self.frames)))\n",
    "\n",
    "\n",
    "        # self.transform = transforms.Compose([\n",
    "        #     transforms.Resize((84, 84)),\n",
    "        #     transforms.ToTensor(),\n",
    "        # ])\n",
    "\n",
    "    # use custom process instead of torchvision transforms, this way we dont need to install torchvision\n",
    "    # for only its transforms. They should effectively do the same thing\n",
    "    def process_image(self, img, resize_to=(140, 114)):\n",
    "        img = img.resize(resize_to, Image.Resampling.LANCZOS)\n",
    "        img_array = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        img_tensor = torch.tensor(img_array).unsqueeze(0)  # Add channel dimension (1, H, W)\n",
    "\n",
    "        return img_tensor # no division (/255) needed because model does it, TODO: maybe move it to procesing instead of model\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.valid_indices[idx]\n",
    "        imgs = []\n",
    "        for i in range(idx - self.frame_window, idx):\n",
    "            frame_key = self.frames[i]\n",
    "            img_path = os.path.join(self.image_folder, f\"d_{frame_key}.png\")\n",
    "            img = Image.open(img_path).convert(\"L\")\n",
    "            imgs.append(self.process_image(img))\n",
    "\n",
    "\n",
    "\n",
    "        state = torch.cat(imgs, dim=0)  # [3*4, 84, 84]\n",
    "        #print(\"state shape\", state.shape)\n",
    "\n",
    "        frame_key = self.frames[idx]\n",
    "        keys = {k: v for k, v in self.movements[frame_key].items() if k != \"state\"}\n",
    "        actions_dict = agent_action(keys)\n",
    "        #print(\"action-dict\", actions_dict)\n",
    "\n",
    "        # TODO check if this is always correct, I think so\n",
    "        # idea here to get the valid action, we take the last if there are more\n",
    "        # e.g. jump_right is after jump and move/sprint right\n",
    "        # latest action counts\n",
    "        for a, value in reversed(actions_dict.items()):\n",
    "            if value:\n",
    "                action = a\n",
    "\n",
    "\n",
    "        reward = self.movements[frame_key][\"state\"].get(\"reward\", 0.0)\n",
    "\n",
    "        # Next frame\n",
    "        if idx + 1 < len(self.frames):\n",
    "            next_frame_key = self.frames[idx + 1]\n",
    "            next_img_path = os.path.join(self.image_folder, f\"d_{next_frame_key}.png\")\n",
    "            next_img = Image.open(next_img_path).convert(\"L\")\n",
    "            next_state = torch.cat([\n",
    "                *imgs[1:],\n",
    "                self.process_image(next_img)\n",
    "            ], dim=0)\n",
    "        else:\n",
    "            next_state = state\n",
    "\n",
    "        return state, action, reward, next_state\n",
    "    \n",
    "\n",
    "# -- testing dataset functionality --\n",
    "\n",
    "\n",
    "# TODO: change the paths here to the ones you have\n",
    "dataset = MarioDataset(\n",
    "    image_folder=\"../data/screenshots\",\n",
    "    movements=json.load(open(\"../data/movements.json\")),\n",
    "    frame_window=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f3917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 114, 140])\n"
     ]
    }
   ],
   "source": [
    "data0 = dataset[0]\n",
    "print(data0[0].shape)  # [3*4, 84, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe89187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def train(dataset, epochs=1, batch_size=32, learning_rate=0.0001, gamma=0.99):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = BTRNetwork(num_actions=NUM_ACTIONS).to(device)\n",
    "    model.train()\n",
    "\n",
    "    target_model = BTRNetwork(num_actions=NUM_ACTIONS).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i, (states, actions, rewards, next_states) in enumerate(dataloader):\n",
    "\n",
    "            #print(type(states), type(actions), type(rewards), type(next_states))\n",
    "                \n",
    "    \n",
    "            actions_discrete = [ACTION_TO_INDEX[a] for a in actions]\n",
    "            actions = torch.tensor(actions_discrete, dtype=torch.long).to(device)\n",
    "\n",
    "            states = states.float().to(device)\n",
    "            #print(\"states shape\", states.shape)\n",
    "            rewards = rewards.float().to(device)\n",
    "            next_states = next_states.float().to(device)\n",
    "\n",
    "            # Configs for Munchausen RL\n",
    "            alpha = 0.9  # log-policy scaling\n",
    "            tau = 0.03   # softmax temperature\n",
    "            clip_min = -1.0  # munchausen term clipped minimum\n",
    "\n",
    "            # Get current Q-values\n",
    "            q_values = model(states)\n",
    "            #print(\"q_values shape\", q_values.shape)\n",
    "            q_values_mean = q_values.mean(dim=1)\n",
    "            #print(\"q_values_mean shape\", q_values_mean.shape)\n",
    "            q_taken = q_values_mean.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            #print(\"q_taken shape\", q_taken.shape)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q = target_model(next_states)\n",
    "                #print(\"next_q shape\", next_q.shape)\n",
    "                next_q_mean = next_q.mean(dim=1)\n",
    "                #print(\"next_q_mean shape\", next_q_mean.shape)\n",
    "                next_probs = F.softmax(next_q_mean / tau, dim=1)       # π(a|s')\n",
    "                #print(\"next_probs shape\", next_probs.shape)\n",
    "                next_v = (next_q_mean * next_probs).sum(dim=1)  # Shape: [32]\n",
    "                #print(\"next_v shape\", next_v.shape)\n",
    "\n",
    "                # Softmax over current Q-values (for log π(a|s))\n",
    "                current_probs = F.softmax(q_values_mean / tau, dim=1)\n",
    "                log_policy = torch.log(current_probs + 1e-8)       # [B, A]\n",
    "                log_pi_a = log_policy.gather(1, actions.unsqueeze(1)).squeeze(1)  # log π(a|s)\n",
    "                munchausen_term = alpha * torch.clamp(log_pi_a, min=clip_min)\n",
    "\n",
    "                # Munchausen target\n",
    "                target_q = rewards + munchausen_term + gamma * next_v\n",
    "\n",
    "            loss = loss_fn(q_taken, target_q)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # After each epoch\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11963593",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model, losses) = train(dataset, epochs=1, batch_size=32, learning_rate=0.0004, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49ae5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190.75753784179688, 107.46211242675781, 279.993896484375, 1017.7136840820312, 1600.1640625, 765.0189819335938, 757.6370849609375, 275.71575927734375, 195.26783752441406, 106.71176147460938, 46.86139678955078, 846.6123046875, 129.99386596679688, 484.776123046875, 181.86083984375, 34.32994842529297, 327.1317443847656, 1657.4541015625, 569.2169799804688, 45.79926300048828, 531.7164916992188, 20.512714385986328, 296.3560485839844, 562.9819946289062, 689.1611328125, 357.585693359375, 175.3204345703125, 218.06874084472656, 306.75775146484375, 147.36978149414062, 14.530933380126953, 520.7408447265625, 42.95661163330078, 17.623231887817383, 69.87158203125, 300.6549072265625, 22.28498077392578, 23.362449645996094, 908.6766357421875, 148.87484741210938, 107.35952758789062, 81.02940368652344, 712.8294677734375, 29.169862747192383, 228.8233642578125, 152.81634521484375, 110.84651184082031, 35.4313850402832, 93.10283660888672, 74.55778503417969, 328.21514892578125, 683.56396484375, 69.82035827636719, 185.13134765625]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45a4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "#torch.save(model.state_dict(), \"btr_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
