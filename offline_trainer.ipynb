{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40924aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# --- Constants ---\n",
    "ACTION_KEYS = [\"left\", \"right\", \"jump\", \"sprint\"]\n",
    "ACTION_TO_INDEX = {tuple([int(k == a) for k in ACTION_KEYS]): i for i, a in enumerate(ACTION_KEYS)}\n",
    "NUM_ACTIONS = len(ACTION_KEYS)\n",
    "\n",
    "# --- Dataset ---\n",
    "class MarioDataset(Dataset):\n",
    "    def __init__(self, image_folder, movements, frame_window=4):\n",
    "        self.image_folder = image_folder\n",
    "        self.movements = movements\n",
    "        self.frame_window = frame_window\n",
    "        self.frames = sorted(movements.keys())\n",
    "        self.valid_indices = list(range(frame_window, len(self.frames)))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((84, 84)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.valid_indices[idx]\n",
    "        imgs = []\n",
    "        for i in range(idx - self.frame_window, idx):\n",
    "            frame_key = self.frames[i]\n",
    "            img_path = os.path.join(self.image_folder, f\"{frame_key}.png\")\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            imgs.append(self.transform(img))\n",
    "\n",
    "        state = torch.cat(imgs, dim=0)  # [3*4, 84, 84]\n",
    "\n",
    "        # Action (convert multi-hot to index)\n",
    "        frame_key = self.frames[idx]\n",
    "        action_state = self.movements[frame_key][\"state\"]\n",
    "        action_vec = tuple([int(action_state[k]) for k in ACTION_KEYS])\n",
    "        action = ACTION_TO_INDEX.get(action_vec, 0)\n",
    "\n",
    "        reward = self.movements[frame_key][\"state\"].get(\"reward\", 0.0)\n",
    "\n",
    "        # Next frame\n",
    "        if idx + 1 < len(self.frames):\n",
    "            next_frame_key = self.frames[idx + 1]\n",
    "            next_img_path = os.path.join(self.image_folder, f\"{next_frame_key}.png\")\n",
    "            next_img = Image.open(next_img_path).convert(\"RGB\")\n",
    "            next_state = torch.cat([\n",
    "                *imgs[1:],\n",
    "                self.transform(next_img)\n",
    "            ], dim=0)\n",
    "        else:\n",
    "            next_state = state\n",
    "\n",
    "        return state, action, reward, next_state\n",
    "\n",
    "# --- Simple Q-Network (based on BTR) ---\n",
    "class BTRNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_q_learning(dataset, model_path, epochs=5, batch_size=32, gamma=0.99):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = BTRNetwork(input_channels=12, num_actions=NUM_ACTIONS).to(device)\n",
    "    target_model = BTRNetwork(input_channels=12, num_actions=NUM_ACTIONS).to(device)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for state, action, reward, next_state in dataloader:\n",
    "            state = state.to(device)\n",
    "            next_state = next_state.to(device)\n",
    "            action = action.to(device)\n",
    "            reward = reward.to(device)\n",
    "\n",
    "            q_values = model(state)\n",
    "            next_q_values = target_model(next_state)\n",
    "            max_next_q = next_q_values.max(dim=1)[0]\n",
    "            targets = q_values.clone().detach()\n",
    "            for i in range(state.size(0)):\n",
    "                targets[i, action[i]] = reward[i] + gamma * max_next_q[i]\n",
    "\n",
    "            loss = loss_fn(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# --- Pseudo-code for online usage ---\n",
    "ONLINE_USAGE_PSEUDO = \"\"\"\n",
    "# Load model\n",
    "model = BTRNetwork(input_channels=12, num_actions=4)\n",
    "model.load_state_dict(torch.load(\"btr_q_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Inside emulator loop:\n",
    "obs = get_4frame_stack_from_emulator()  # shape [12, 84, 84]\n",
    "action_values = model(obs.unsqueeze(0))\n",
    "action = action_values.argmax().item()\n",
    "\n",
    "# Send action to emulator\n",
    "send_action_to_emulator(action)\n",
    "\n",
    "# Collect new (s, a, r, s') and store in new buffer\n",
    "# Optionally fine-tune with Q-learning\n",
    "\"\"\"\n",
    "\n",
    "# --- Main driver ---\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"movements.json\", \"r\") as f:\n",
    "        movements = json.load(f)\n",
    "    dataset = MarioDataset(\"screenshots\", movements)\n",
    "    train_q_learning(dataset, \"btr_q_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
